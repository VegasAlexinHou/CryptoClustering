# Import required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


# Load the data into a Pandas DataFrame and make the index the "coin_id" column.
market_data_df = pd.read_csv("Resources/crypto_market_data.csv", index_col="coin_id")

# Display sample data
market_data_df.head(10)


# Generate summary statistics
market_data_df.describe()





# Use the `StandardScaler()` module from scikit-learn to normalize the data from the CSV file
# Initialize the StandardScaler
scaler = StandardScaler()

# Normalize the data in the DataFrame
normalized_data = scaler.fit_transform(market_data_df)

# Convert the normalized data back to a DataFrame
normalized_df = pd.DataFrame(normalized_data, columns=market_data_df.columns, index=market_data_df.index)

# Display the normalized data
normalized_df.head(10)



# Create a DataFrame with the scaled data


# Copy the crypto names from the original data


# Set the coinid column as index


# Display sample data

import pandas as pd
from sklearn.preprocessing import StandardScaler

def normalize_data(data_df):
    # Initialize the StandardScaler
    scaler = StandardScaler()
    
    # Normalize the data in the DataFrame
    normalized_data = scaler.fit_transform(data_df)
    
    # Convert the normalized data back to a DataFrame
    normalized_df = pd.DataFrame(normalized_data, columns=data_df.columns, index=data_df.index)
    
    return normalized_df

# Load the data into a Pandas DataFrame and make the index the "coin_id" column.
market_data_df = pd.read_csv("Resources/crypto_market_data.csv", index_col="coin_id")

# Display sample data
market_data_df.head(10)

# Normalize the data
normalized_df = normalize_data(market_data_df)

# Display the normalized data
normalized_df.head(10)






# Create a list with the number of k-values to try
# Use a range from 1 to 11


# Create an empty list to store the inertia values


# Create a for loop to compute the inertia with each possible value of k
# Inside the loop:
# 1. Create a KMeans model using the loop counter for the n_clusters
# 2. Fit the model to the data using the scaled DataFrame
# 3. Append the model.inertia_ to the inertia list


# Create a dictionary with the data to plot the Elbow curve


# Create a DataFrame with the data to plot the Elbow curve


# Display the DataFrame

# Create a list with the number of k-values to try
# Use a range from 1 to 11
k_values = range(1, 11)

# Create an empty list to store the inertia values
inertia_values = []

# Create a for loop to compute the inertia with each possible value of k
for k in k_values:
    # Create a KMeans model using the loop counter for the n_clusters
    kmeans = KMeans(n_clusters=k)
    
    # Fit the model to the data using the scaled DataFrame
    kmeans.fit(normalized_df)
    
    # Append the model.inertia_ to the inertia list
    inertia_values.append(kmeans.inertia_)

# Create a dictionary with the data to plot the Elbow curve
elbow_data = {'k_values': k_values, 'inertia_values': inertia_values}

# Create a DataFrame with the data to plot the Elbow curve
elbow_df = pd.DataFrame(elbow_data)

# Display the DataFrame
print(elbow_df)



# Plot a line chart with all the inertia values computed with 
# the different values of k to visually identify the optimal value for k.

import matplotlib.pyplot as plt

# Plot the line chart
plt.figure(figsize=(10, 6))
plt.plot(elbow_df['k_values'], elbow_df['inertia_values'], marker='o', color='b', linestyle='-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Curve to Determine Optimal k')
plt.grid(True)
plt.show()









# Initialize the K-Means model using the best value for k
# Initialize the K-Means model with k=4
best_kmeans_model = KMeans(n_clusters=4)

# Fit the model to the normalized data
best_kmeans_model.fit(normalized_df)



# Fit the K-Means model using the scaled data
#best_kmeans_model.fit(normalized_df)


# Predict the clusters for the normalized data
cluster_labels = best_kmeans_model.predict(normalized_df)

# View the resulting array of cluster values
print(cluster_labels)



# Create a copy of the DataFrame
normalized_df_copy = normalized_df.copy()


# Add a new column to the DataFrame with the predicted clusters
normalized_df_copy['cluster'] = cluster_labels

# Display sample data with the new 'cluster' column
normalized_df_copy.head(10)



# Create a scatter plot using Pandas plot by setting 
# `x="price_change_percentage_24h"` and `y="price_change_percentage_7d"`.
# Use "rainbow" for the color to better visualize the data.

# Create a scatter plot using Pandas plot
normalized_df_copy.plot(kind='scatter', x='price_change_percentage_24h', y='price_change_percentage_7d', c='cluster', colormap='rainbow', figsize=(10, 6))
plt.xlabel('Price Change Percentage (24h)')
plt.ylabel('Price Change Percentage (7d)')
plt.title('Cluster Analysis Scatter Plot')
plt.grid(True)
plt.show()






# Create a PCA model instance and set `n_components=3`.
pca = PCA(n_components=3)

# Fit the PCA model to the normalized data
pca.fit(normalized_df)

# Transform the data using the PCA model
pca_data = pca.transform(normalized_df)

# Convert the transformed data back to a DataFrame
pca_df = pd.DataFrame(data=pca_data, columns=['PC1', 'PC2', 'PC3'])
pca_df['coin_id']= normalized_df.index
pca_df = pca_df.set_index('coin_id')

# Display the transformed data
pca_df.head(10)


# Use the PCA model with `fit_transform` on the original scaled DataFrame to reduce to three principal components.
#pca_data_original = pca.fit_transform(normalized_df)

# View the first five rows of the DataFrame. 
#pca_df_original = pd.DataFrame(data=pca_data_original, columns=['PC1', 'PC2', 'PC3'], index=normalized_df.index)

#print(pca_df_original.head(5))


# Retrieve the explained variance to determine how much information  can be attributed to each principal component.
# Retrieve the explained variance from the PCA model
explained_variance = pca.explained_variance_ratio_

# Create a DataFrame to display the explained variance for each principal component
explained_variance_df = pd.DataFrame(data=explained_variance, columns=['Explained Variance'], index=['PC1', 'PC2', 'PC3'])

# Display the explained variance for each principal component
print(explained_variance_df)





# Create a new DataFrame with the PCA data.
# Note: The code for this step is provided for you

# Creating a DataFrame with the PCA data


# Copy the crypto names from the original data


# Set the coinid column as index


# Display sample data

#Done in steps before
# Display sample data
pca_df.head(10)






# Create a list with the number of k-values to try
# Use a range from 1 to 11


# Create an empty list to store the inertia values


# Create a for loop to compute the inertia with each possible value of k
# Inside the loop:
# 1. Create a KMeans model using the loop counter for the n_clusters
# 2. Fit the model to the data using PCA DataFrame.
# 3. Append the model.inertia_ to the inertia list


# Create a dictionary with the data to plot the Elbow curve


# Create a DataFrame with the data to plot the Elbow curve


# Display the DataFrame

# Create a list with the number of k-values to try
# Use a range from 1 to 11
k_values = range(1, 11)

# Create an empty list to store the inertia values
inertia_values = []

# Create a for loop to compute the inertia with each possible value of k
for k in k_values:
    # Create a KMeans model using the loop counter for the n_clusters
    kmeans = KMeans(n_clusters=k,n_init='auto',random_state=1)
    
    # Fit the model to the data using the PCA DataFrame
    kmeans.fit(pca_df)
    
    # Append the model.inertia_ to the inertia list
    inertia_values.append(kmeans.inertia_)

# Create a dictionary with the data to plot the Elbow curve
elbow_data = {'k_values': k_values, 'inertia_values': inertia_values}

# Create a DataFrame with the data to plot the Elbow curve
elbow_df = pd.DataFrame(elbow_data)

# Display the DataFrame
print(elbow_df)



# Plot a line chart with all the inertia values computed with 
# the different values of k to visually identify the optimal value for k.
# Plot the line chart
plt.figure(figsize=(10, 6))
plt.plot(elbow_df['k_values'], elbow_df['inertia_values'], marker='o', color='b', linestyle='-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Curve to Determine Optimal k')
plt.grid(True)
plt.show()









# Initialize the K-Means model using the best value for k
# Initialize the K-Means model using the best value for k
best_k = 3  # Set the best value for k based on the Elbow curve analysis

# Initialize the K-Means model with the best value of k
best_kmeans_model = KMeans(n_clusters=best_k)

# Fit the model to the PCA data
best_kmeans_model.fit(pca_df)

# Predict the clusters for the PCA data
cluster_labels = best_kmeans_model.predict(pca_df)

# Add a new column to the PCA DataFrame with the predicted clusters
pca_df['cluster'] = cluster_labels

# Display sample data with the new 'cluster' column
print(pca_df.head(10))


# Fit the K-Means model using the PCA data
best_kmeans_model.fit(pca_df)

# Predict the clusters for the PCA data
cluster_labels = best_kmeans_model.predict(pca_df)

# Add a new column to the PCA DataFrame with the predicted clusters
pca_df['cluster'] = cluster_labels

# Display sample data with the new 'cluster' column
print(pca_df.head(10))


# Predict the clusters to group the cryptocurrencies using the PCA data
cluster_labels = best_kmeans_model.predict(pca_df)

# Print the resulting array of cluster values.
print(cluster_labels)



# Create a copy of the DataFrame with the PCA data
pca_df_copy = pca_df.copy()

# Add a new column to the DataFrame with the predicted clusters
pca_df_copy['cluster'] = cluster_labels

# Display sample data
print(pca_df_copy.head(10))


# Create a scatter plot using hvPlot by setting `x="PCA1"` and `y="PCA2"`. 
import hvplot.pandas

# Create a scatter plot using hvPlot
pca_df_copy.hvplot.scatter(x='PC1', y='PC2', by='cluster', hover_cols=['coin_id'], height=500, width=800)





# Use the columns from the original scaled DataFrame as the index.
# Retrieve the principal components from the PCA model
#principal_components = pca.components_

# Create a DataFrame to display the weights of each feature on each principal component
#weights_df = pd.DataFrame(principal_components, columns=normalized_df.columns, index=['PC1', 'PC2', 'PC3'])

# Display the weights of each feature on each principal component
#print(weights_df)

# Identify the features with the strongest positive or negative influence on each component
#for component in weights_df.index:
    #print(f"Principal Component: {component}")
    #component_weights = weights_df.loc[component]
    
    # Sort the feature weights in descending order to identify the strongest positive and negative influences
   # sorted_weights = component_weights.sort_values(ascending=False)
    
    # Print the top 3 features with the strongest positive and negative influences
   # print("Top 3 features with strongest positive influence:")
  #  print(sorted_weights.head(3))
    
   # print("Top 3 features with strongest negative influence:")
  #  print(sorted_weights.tail(3))


# Use the columns from the original scaled DataFrame as the index.
# Retrieve the principal components from the PCA model
principal_components = pca.components_.T

# Create a DataFrame to display the weights of each feature on each principal component
weights_df = pd.DataFrame(principal_components, columns=['PC1', 'PC2', 'PC3'],index=normalized_df.columns)

weights_df






